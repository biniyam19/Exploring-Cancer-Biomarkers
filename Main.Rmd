---
author: "Biniyam Gebeyehu"
output:
  pdf_document:
    number_sections: true
always_allow_html: true
---
\begin{titlepage}
   \begin{center}
       \vspace*{1cm}
       \
       \textbf{\Large{Exploring Relationship Breast Cancer Biomarkers and Diagnosis}}

       \vspace{5cm}
       


       \vspace{8.5cm}

       \textbf{\large{Prepared by:}}\\
       \vspace{0.2cm}
       \textbf{\large{Biniyam Gebeyehu}}
       
       \vfill
            

       \vspace{0.8cm}
    
   \end{center}
\end{titlepage}

<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
      max-width: 800px;
      text-align: left;
      margin-left: 0.4in;
      margin-right: 0.4in;
  }
td {  /* Table  */
  font-size: 2px;
  margin-left: 0.4in;
  margin-right: 0.4in;
}
h1.title {
  font-size: 24px;
  color: DarkRed;
  text-align: center;
  margin-left: 0.4in;
  margin-right: 0.4in;
  margin-top: 7in;
  margin-bottom: 7in;
}
h1 { /* Header 1 */
  font-size: 20px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 18px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 14px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
  text-align: center;

}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

<style type="text/css">
div.main-container {
  max-width: 1600px;
  margin-left: auto;
  margin-right: auto;
  text-align: justify;
}

</style>
<center> </center>

```{css, echo=FALSE}
.header-section-number::after {
  content: ".";
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objectives

Objective of this task is to derive a cutoff of one of the features and to find a combination of potentially diagnostic variables for the identification of benign vs. malignant tumors.

# Dataset

The dataset consists of 569 examples of breast cancer biopsies. Each example has 32 variables. Out of the 32 variables, 30 are continuous features(biomarkers), one is an identification number, and the other is diagnosis. The diagnosis outcome is coded as "M" to indicate the tumor is malignant and as "B" to indicate the tumor is benign.


```{r, echo=FALSE, warning = FALSE, message = FALSE}
#Loading libraries
library(dplyr)
library(tidyr)
library(caret)
library(cutpointr)
library(lemon)
library(ModelGood)
library(easyGgplot2)
library(moments)
library(kableExtra)
library(reactable)
library(captioner)
library(ggplot2)
library(hrbrthemes)
library(viridis)
library(gridExtra)
library(ggplot2)
library(grid)
library(reshape2)
library(tidyverse)
library(hrbrthemes)
library(viridis)
library(FactoMineR)
library(factoextra)
require(ggpubr)
require(tidyverse)
require(Hmisc)
require(corrplot)
library(randomForest)
library(faraway)
library(lemon)
library(FSelector)
```


```{r, echo=FALSE, warning = FALSE, message = FALSE}
#importing the dataset
breast_cancer_df <- read.csv("DataScientist_test_data.csv")
breast_cancer_df <- subset(breast_cancer_df, select = -X )
```


# Exploratory Data Analysis(EDA)

Exploratory analysis was performed to describe the centrality, variability, and distributional behavior of the biomarkers. The mean was used as a measurement of central tendency, the standard deviation, range, and the coefficient of variation(CV) were used as measurements of variation, and skewness and kurtosis were used as measurements of distribution shape. Also, as part of data exploration histogram of each biomarker by diagnosis (M vs. B) was visualized.  



### Summary Statistics {-}


```{r, echo=FALSE, warning = FALSE, message = FALSE,   height = 10 }
#creating an empty data frame with four columns
summary_stat <-
  data.frame(
    Mean = double(),
    StDev = double(),
    Skewness = double(),
    Kurtosis = double()
  )

#Creating empty variables for storing the descriptive measures: mean, standard deviation, coefficient of variation, range, skewnwss, and kurtosis
Mean = NULL; StDev = NULL; CV = NULL; Range = NULL; Skewness = NULL; kurtosis = NULL

#Loop over numeric columns and compute the descriptive measures
for (i in 3:32){
  Mean <- append(Mean,round(mean(breast_cancer_df[,i]), 3))
  StDev <- append(StDev, round(sd(breast_cancer_df[,i]),3))
  CV <- append(CV,round(sd(breast_cancer_df[,i])/mean(breast_cancer_df[,i]),3)*100)
  Skewness <- append(Skewness, round(skewness(breast_cancer_df[,i]),3)) 
  kurtosis <- append(kurtosis, round(kurtosis(breast_cancer_df[,i]),3))
  Range <- append(Range, round(diff(range(breast_cancer_df[,i])),3))
}

#Make a data frame from the computed measures
summary_stat <-
  data.frame(Mean, StDev, CV, Range, Skewness, kurtosis)
rownames(summary_stat) <- colnames(breast_cancer_df)[3:32]

#Printing the computed descriptive statistics
kbl(summary_stat, booktabs = T,   longtable = T, caption = "Descreptive statistics") %>% kable_styling(c("striped", "condensed"), full_width = F, latex_options = c("HOLD_position", "scale_down", "striped"),   font_size = 6.5) 

```


### Histogram of Biomarkers {-}

```{r, echo=FALSE,fig.width=12,fig.height=12, fig.cap=paste("Histogram of biomarkers by diagnosis"), warning = FALSE, message = FALSE, fig.align = 'center', , fig.pos="h"}

#Plot histogram for each biomarker by diagnosis
#Define an empty list to store figures
hist_plot_list = list()

#Selct biomerkers and put them in a new data frame
df_for_plot <- subset(breast_cancer_df, select = -c(id, diagnosis))

#iterate over columns and make a histogram by diagnosis for each columns
for (i in 1:dim(df_for_plot)[2]) {
  df_plot = data.frame(df_for_plot[, i])
  colnames(df_plot) <- c("plot_now")
  df_plot$diagnosis <- breast_cancer_df$diagnosis
  # Making a histogram for every biomarkers 
  p = ggplot(df_plot, aes(x = plot_now, color = diagnosis)) + geom_density(alpha =.3) +
    geom_histogram(
      aes(y = ..density..),
      alpha = 0.5,
      bins = 40,
      show.legend = FALSE
    ) +
    #changing the default colors
    scale_colour_manual(values = c("M" = "blue", "B" = "red")) + xlab(colnames(df_for_plot)[i])+
    labs(y = NULL) + theme(legend.position = "none") + theme(axis.text =
                                                        element_text(size = 5),
                                                        axis.title = element_text(size=7))
  
  # extracting the legend from the the first figure to be used as a common legend
  if (i == 1) {
    legend <- g_legend(p + theme(legend.position = "top"))
  }
  hist_plot_list[[i]] = p #adding the plot to the list
}

#Presenting 30 histograms together
grid.arrange(
  grobs = c(hist_plot_list[!sapply(hist_plot_list, is.null)], list(legend)),
  left = "Density",
  element_text(size = 3),
  gp = gpar(cex = 1),
  ncol = 6
)
```


According to the CV values, area_se (standard error of area), concavity_se(standard error of concavity), and concavity_mean(mean concavity) have the highest relative dispersion compared to other biomarkers.  
<br/> For a variable that has a perfect normal distribution, the skewness is 0 and the kurtosis is 3. From the values of skewness and kurtosis presented in Table 1, all biomarkers have distributions that are deviated from a normal distribution. However, their level of the deviations are different. For example, area_se (standard error of area) has a distribution that least resembles a normal distribution (skewness = 5.433 and kurtosis = 51.767). The biomarker that has the most close to normal distribution is smoothness_worst(skewness =0.414 and Kurtosis = 3.503). The histograms in Figure 1 also showed the deviations of the distributions of biomarkers from a normal distribution for malignant and benign tumors separately.  


# Correlation Between Biomarkers

One important step in predicting a certain outcome is analyzing the relationship between between features. This helps to see if candidate predictors are correlated or if they contain redundant information. To assess the relationship between biomarkers the Spearman's rank correlation coefficients were calculated. 

```{r, echo=FALSE,fig.width=18,fig.height=18,fig.cap=paste("Spearman's rank correlation coefficients"), warning = FALSE, message = FALSE, fig.align='center'}
#Computing spearman correlation
spearman_corr_values = rcorr(as.matrix(breast_cancer_df[, 3:32]), type = "spearman")

#Extracting the correlation coefficient and the p-values
M <- spearman_corr_values$r
p_mat <- spearman_corr_values$P
col <-
  colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

#Visualizing the correlation values
M <- spearman_corr_values$r
p_mat <- spearman_corr_values$P
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method = "color", col = col(30),
         type = "upper", order = "hclust",
         addCoef.col = "black", # Add coefficient of correlation
         tl.col = "darkblue", tl.srt = 45, #Text label color and rotation
         # Combine with significance level
         p.mat = p_mat, sig.level = 0.01,
         # hide correlation coefficient on the principal diagonal
         diag = FALSE, insig = "blank", tl.cex = 1.6
         )
```
Pearson correlation coefficient is best when two variables have a bivariate normal distribution which seems less logical to assume for the biomarkers we have considering the results presented in the previous section. One alternative is to apply non-parametric methods such as Spearman's rank correlation. 
As presented in Figure 3, there is strong correlation between features that represent area, diameter, perimeter, and concave points. The minimum observed pairwise correlation between them is 0.4. In the heatmap, uncolored cells represents correlation values that are not significantly different from zero at 1% level of significance.    


### Comparing Means of Biomarkers by Diagnosis{-}

```{r, echo=FALSE, warning = FALSE, message = FALSE}
#applying to t-test to compare the biomarkers mean by diagnosis
test_result <- breast_cancer_df[, 2:32] %>%
  select_if(is.numeric) %>%
  map_df(~ broom::tidy(t.test(. ~ breast_cancer_df$diagnosis)), .id = 'var') %>% mutate_if(is.numeric, round, 3)

#Selcting columns that are needed
Main_content = test_result[, c('var',
                               'estimate1',
                               'estimate2',
                               'statistic',
                               'p.value',
                               'conf.low',
                               'conf.high')]

#renaming the columns of selected columns of the t-test
names(Main_content) <-
  c(
    'Variable',
    'Benign_Mean',
    'Malignant_Mean',
    'Statistic',
    'P.value',
    'Conf.Low',
    'Conf.High'
  )

# rounding numeric columns to five digit
Main_content <- Main_content %>% mutate_if(is.numeric, round, 3)

# Presenting the t-test summary in table form
kbl(
  Main_content,
  digits = 3,
  booktabs = T,
  caption = "T-test mean of biomarkers by diagnosis",
  table.attr = "style='width:100%;'"
)  %>% kable_styling(full_width = F) %>% kable_styling(c("striped", "condensed"), full_width = F)%>%kable_styling(latex_options = "HOLD_position")



```

T-test was used to determine if the mean biomarker values for malignant and benign tumors are statistically equal. Since the sample size is large (>30) the distribution of the t-test statistic will have approximately a normal distribution as given by the Central Limit Theorem(CLT) irrespective of the distribution of the population.
The result of the t-test is presented in Table 2. At a 1% level of significance, the mean values for malignant and benign tumors are not significantly different for only four of the biomarkers: fractal_dimension_mean, fractal_dimension_se, texture_se, smoothness_se, and symmetry_se.

\newpage

### Principal Component Analysis(PCA) {-}

PCA was used to further explore the relationship between biomarkers. After applying PCA, the relationship between principal components and biomarkers was analyzed to identify biomarkers that are correlated each other most and biomarkers that contributed most to the total observed variation.


```{r, echo=FALSE,fig.cap=paste("Scree plot"), fig.width=5,fig.height=3.5, warning = FALSE, message = FALSE, fig.align='center', fig.margin = FALSE}
# selecting numeric columns from the original dataset then applying PCA
pca_df = breast_cancer_df[, 3:32]
pca_result <-
  PCA(pca_df,
      scale.unit = TRUE,
      ncp = 30,
      graph = FALSE)

#Make a scree plot
fviz_eig(pca_result, addlabels = TRUE)
```
```{r, echo=FALSE}
### Principal Components and Diagnosis {-}
```

```{r, echo=FALSE, fig.cap=paste("Scatter plot of the first two principal components"), fig.width=6,fig.height=3.5, warning = FALSE, message = FALSE, fig.align='center', fig.pos="H", fig.margin = FALSE}
# Preparing a data frame from the first two components of the PCA and dignosis
pca_df = data.frame(pca_result$ind$coord[, 1])
colnames(pca_df) <- ("PC1")
pca_df$PC2 <- pca_result$ind$coord[, 2]
pca_df$diagnosis = breast_cancer_df$diagnosis

# Presenting the PC1 and PC2 in scatter plot
p = ggplot(pca_df, aes(x = PC1, y = PC2, color = diagnosis)) + geom_point() +
  scale_colour_manual(values = c("M" = "blue", "B" = "red")) + xlab("PC1 (44.3%)") + ylab("PC2
  (19.0%)") + geom_vline(xintercept = 0,
                         linetype = "dotted",
                         lwd = 1) + geom_hline(yintercept = 0,
                                               linetype = "dotted",
                                               lwd = 1) + theme_light()
p
```

```{r, echo=FALSE, warning = FALSE, message = FALSE}
#Scaling biomarkers before applying PCA
breast_cancer_df_scaled <-
  data.frame(lapply(breast_cancer_df[, 3:32], function(x)
    c(scale(x))))

#Extracting PCs and multiplying them by explained variance for better visualization
PCA_df = pca_result$ind$coord
colnames(PCA_df) <- NULL
PCA_explained_var = as.data.frame(Map('*',
  data.frame(pca_result$ind$coord),
  as.vector(pca_result$eig[, 2])
))
# Scaling the product of PCs and explained variance 
PCA_explained_var_scaled <-
  data.frame(lapply(PCA_explained_var, function(x)
    c(scale(x))))

# Computing the dot product of PCs and features and get a 30x30 matrix
PCA_features_corr <-
  t(as.matrix(breast_cancer_df_scaled)) %*% as.matrix(PCA_explained_var_scaled)
```

### Relationship Between Biomarkers and Principal Components {-}

```{r,  echo=FALSE, fig.cap=paste("Relationship between principal components and features "), fig.width=10,fig.height=8, warning = FALSE, message = FALSE, fig.align='center', fig.pos="h"}
# renaming the column names of the PCs
colnames(PCA_features_corr) <- paste0("PC", 1:30)

#Converting the matrix(that show the relationship between PCs and biomarkers) to long format
PCA_features_corr_df <- melt(PCA_features_corr)
colnames(PCA_features_corr_df) <- c("Biomarkers", "PC", "value")

# Making heatmap for the matrix that show the rship between PCs and biomarkers
p = ggplot(PCA_features_corr_df, aes(x = PC , y = Biomarkers, fill = value)) +
  geom_tile() + scale_fill_viridis() + theme(text = element_text(size = 12),
                                             axis.text.x = element_text(angle = 60, hjust = 1))
p

```

As presented in Figure 4, the first Principal Component (PC) accounts for 44.3% of variance from the original set of biomarkers. Similarly, all subsequent orthogonal components account for the maximum proportion of the remaining variance. Usually the orthogonal components of PCA doesn't help in searching features that best predict a certain outcome variable. This is mainly because PCA do the transformation irrespective of the outcome variable. However for breast cancer biomarkers, the orthogonal components magnified the boundaries between malignant and benign groups, see Figure 5.  
Figure 6 presents the relationship between biomarkers and features. It was computed by taking the dot product of features and orthogonal components. The figure tells which biomarkers contributed for which principal component. For example, the five major contributors of the first principal are: concave.points_mean, concave.points_worst, concavity_mean, perimeter_worst, and compactness_mean. 


\newpage
# Modeling Diagnosis Outcome

Two models were developed to be able to recognize the type of the tumor based on biomarkers or features generated from them. The first fitted model is Generalized Linear Model(GLM) and the second model is Random Forest(RF) model. First, the dataset was split as 70% train and 30% test set. For the GLM collinearities between biomarkers were assessed by computing VIF and Tolerance. 


```{r, echo=FALSE, warning = FALSE, message = FALSE}
#Generating index to split the dataset as train and test set
set.seed(321)
trainIndex <-
  createDataPartition(
    breast_cancer_df$diagnosis,
    p = .7,
    list = FALSE,
    times = 1
  )

#Converting string column diagnosis to numeric
breast_cancer_df$diagnosis_numeric <-
  ifelse(breast_cancer_df$diagnosis == "B", 0, 1)

#Defining the train set and the test set
TrainData <- breast_cancer_df[trainIndex, ]
TrainData <- select(TrainData,-id)
TestData <- breast_cancer_df[-trainIndex, ]
TestData <- select(TestData,-id)
```


### Checking the Presence of Multicollinearity {-}


```{r, echo=FALSE, warning = FALSE}
options(digits = 3, scipen = -2)

#Defining the GLM (logistic) model
breast_cancer_glm <-
  glm(factor(diagnosis_numeric) ~ .,
      family = binomial(link = "logit"),
      data = TrainData)

#Creating data frame that contain VIF and Tolerance
Collinearity_stat <- data.frame(vif(breast_cancer_glm))
colnames(Collinearity_stat) <- c("VIF")
Collinearity_stat$Tolerance <-
  formatC(1 / Collinearity_stat$VIF,
          format = "e",
          digits = 2)

#Convert big numbers to scientific numbers
Collinearity_stat$VIF <-
  formatC(Collinearity_stat$VIF, format = "e", digits = 2)

#Concatenating the VIF and Tolerance for better presentation
VIF_Tolerance_df <-
  data.frame(sprintf("%s (%s)", Collinearity_stat$VIF, Collinearity_stat$Tolerance))
colnames(VIF_Tolerance_df) <- c("VIF(Tolerance)")
rownames(VIF_Tolerance_df) <- rownames(Collinearity_stat)

#Dividing the the table into two parts for presentation
#Part 1
VIF_Tolerance_p1 <- data.frame(VIF_Tolerance_df[1:15, ])
colnames(VIF_Tolerance_p1) <- c("VIF(Tolerance)")
rownames(VIF_Tolerance_p1) <- rownames(Collinearity_stat)[1:15]

#Part 2
VIF_Tolerance_p2 <- data.frame(VIF_Tolerance_df[16:30, ])
colnames(VIF_Tolerance_p2) <- c("VIF(Tolerance)")
rownames(VIF_Tolerance_p2) <- rownames(Collinearity_stat)[16:30]

#Presenting the VIF and tolerance in table form
kable(
  list(VIF_Tolerance_p1, VIF_Tolerance_p2),
  booktabs = T,
  caption = "VIF and Tolerance values",
)  %>% kable_styling(c("striped"), full_width = F) %>% kable_styling(latex_options = "HOLD_position")

```

As presented in Table 3, all the VIF values are greater than 10. A VIF value that exceeds 10 indicates the presence of multicollinearity. This is an issue especially for models such as Linear Models (LM) and Generalized Linear Models(GLM). In such cases, the two commonly used approaches are: (1) to drop some of the features, and (2) create orthogonal features by applying methods such as PCA. For the GLM model, PCA components were used as features.

## Fitting Logistic Regression


The VIF and Tolerance values showed the presence of strong collinearity between biomarkers. For the GLM, the presence of multicollinearity causes unstable estimates and inaccurate standard errors which affects confidence intervals and hypothesis tests. For this reason, for logistic regression, the first five components of PCA are used instead of the original biomarkers. 

```{r, echo=FALSE, warning = FALSE, message = FALSE}
#Fitting Logistic regression using PCA
#Creating a data frame from PCs and including diagnosis as new column
PCA_df = data.frame(pca_result$ind$coord)
colnames(PCA_df) <- paste0("PC", 1:30)
PCA_df$diagnosis_numeric <- breast_cancer_df$diagnosis_numeric

#Splitting the principal components as train and test set
TrainData_PCA = PCA_df[trainIndex, ]
TestData_PCA = PCA_df[-trainIndex, ]

#Fitting logistic regression model
logistic_model <-
  glm(factor(diagnosis_numeric) ~ . ,
      data = TrainData_PCA[, c(1:5, 31)],
      family = binomial(link = "logit"))
```

\newpage
### Summary of Logistic Regression Model {-}

\footnotesize
```{r, echo=FALSE}
#Printing the summary of logistic regression
summary(logistic_model)
```


### Performance of the Logistic Regression Model {-}

```{r, echo=FALSE}
#Predicting the diagnosis for train and test set
predicted_train <-
  predict(logistic_model, TrainData_PCA[, 1:5])  # predicted scores
predicted_test <-
  predict(logistic_model, TestData_PCA[, 1:5], type = "response")  # predicted

#making confusion matrix for test set data
confusionMatrix(factor(TestData$diagnosis_numeric),
                factor(round(predicted_test)),
                positive = "1")
```

\normalsize

Since logistic regression assumes weak or no correlation between predictors, the first five principal components are used. The first five principal components account for 84.8% of the total variation. The results of the model summary indicated that, at a 1% level of significance the coefficients of all the five principal components are significantly different from zero. Also, the model performance summary indicated that the fitted logistic regression correctly classified 97.1% of the cases. It was also able to identify 95.3% of true malignant tumor cases and 98.1% of true benign tumor cases.    


## Fitting Random Forest Model

Unlike the LM and GLM models, the random forest doesn't expect the features to be uncorrelated. For this reason and to directly engage with the original content, biomarkers as they are given were used. The importance of features is analyzed to make the model less complex and to rank the biomarkers based on their importance. The final model is fitted using 15 (50%) of the available features.

### Feature Importance Random Forest Model {-}

```{r, echo=FALSE, fig.cap=paste("Random forest feature importance"), fig.align='center'}
#Fitting a random forest model with all features
RF_model <-
  randomForest(
    factor(diagnosis_numeric) ~ .,
    data = TrainData[, 2:32] ,
    ntree = 500,
    mtry = 6,
    importance = TRUE
  )

#Plotting feature importance result
varImpPlot(
  RF_model,
  sort = TRUE,
  n.var = min(30, nrow(RF_model$importance)),
  type = NULL,
  class = NULL,
  scale = TRUE,
  cex = 0.7
)
```
```{r, echo=FALSE}
#Selecting top 15 important features
TrainData$diagnosis_numeric <-
  as.factor(TrainData$diagnosis_numeric)
imp_features <-
  random.forest.importance(diagnosis_numeric ~ ., data = TrainData[, 2:32])

#selecting only biomarkers
important_15 <- cutoff.k(imp_features, k = 15)
```

```{r, echo=FALSE, fig.width=12,fig.height=12}
#Fitting a random forest model with 15 features
RF_model <-
  randomForest(
    factor(diagnosis_numeric) ~ .,
    data = TrainData[, c(important_15, "diagnosis_numeric")] ,
    ntree = 500,
    mtry = 6,
    importance = TRUE
  )

#Predicting for train set and test set
predTrain <-
  predict(RF_model, TrainData[, important_15], type = "class")
predTest <-
  predict(RF_model, TestData[, important_15], type = "class")

```

\newpage

### Evaluating the Performance of the RF Model {-}

```{r, echo=FALSE}
#Preparing model confusion matrix
confusionMatrix(factor(TestData$diagnosis_numeric),
                factor(predTest),
                positive = "1")

```

Summary of the random forest performance metrics indicated that the model correctly classified 93.5% of the cases for the test set. The model was also able to recognize 88.2% of the true malignant tumor cases and 97.1% of benign tumor cases. According to the feature importance, biomarkers related to the size of the tumor such as area, perimeter, concave points, radius are more important.

# Deriving Cutoff for the Mean Area

### Distribution of Mean Area by Diagnosis {-}

```{r, echo=FALSE,fig.cap=paste("Histogram and scatter plot for mean area"), fig.height = 4, fig.width=10, warning = FALSE, message = FALSE, fig.align='center', fig.pos="H"}
# Histogram of mean area by diagnosis results
g_hist = ggplot(breast_cancer_df, aes(x = area_mean, color = diagnosis)) + geom_histogram(
  aes(y = ..density..),
  alpha = 0.5,
  bins = 40,
  show.legend = FALSE
) + geom_density() + scale_colour_manual(values = c("M" = "blue", "B" = "red")) + xlab("area_mean") + ylab("Density")

#Scatter plot of mean area by diagnosis
g_scatter = ggplot(breast_cancer_df, aes(
  x = seq(1, 569),
  y = area_mean,
  color = diagnosis
)) + geom_point() + scale_colour_manual(values = c("M" = "blue", "B" = "red")) + xlab("Index") + ylab("area_mean")

# printing the histogram and scatter plot side by side
grid_arrange_shared_legend(g_hist, g_scatter, ncol = 2)
```

Both the scatter plot and histogram shows that there is no a single mean area value that neatly divides between malignant and benign tumors. To decide on the cutoff value various methods are available depending on what needs to be achieved and other healthcare and budget related factors.


```{r, echo=FALSE, warning = FALSE, message = FALSE}
#Write a code for sensetivity and specificity
CalcSenSpec = function(x, y, n) { 
  #The function accepts three arguments:
  #x = biomarker, 
  #y = diagnosis, 
  #n= no of sensetivity and specificity points to be calculated 
  Sen_temp <- vector()
  Spec_temp <- vector()
  cutoff <- vector()
  for (i in seq(min(breast_cancer_df$area_mean),
                max(breast_cancer_df$area_mean),
                length.out = n)) {
    Sen_temp <- append(Sen_temp, Sensitivity(x, y, i)$Sensitivity)
    Spec_temp <- append(Spec_temp, Specificity(x, y, i)$Specificity)
    cutoff <- append(cutoff, i)
  }
  # the function returns the sensitivity specificity at different cutoff values
  return(list(Sen_temp, Spec_temp, cutoff))
}
```

```{r, echo=FALSE, warning = FALSE, message = FALSE}
#Compute the sensitivity and specificity
SenSpec_list = CalcSenSpec(breast_cancer_df$area_mean, breast_cancer_df$diagnosis, 1000)
```

### Computing Youden's Index {-}

```{r, fig.height = 3.5, fig.width=6, echo=FALSE, fig.cap=paste("ROC curve for mean area"), warning = FALSE, message = FALSE, fig.align='center', fig.pos="H"}
# Extracting the sensitivity, specificity and the corresponding cutoff values 
# Then make it a data frame
SensSpec_df <-
  data.frame(SenSpec_list[[1]], SenSpec_list[[2]], SenSpec_list[[3]])
colnames(SensSpec_df) <- c("Sensitivity", "Specificity", "cut_off")
SensSpec_df["Index"] = seq(0, 1, length.out = dim(SensSpec_df)[1])

# Computing Youden's index values
Youden_Index = SensSpec_df$Sensitivity + SensSpec_df$Specificity - 1
SensSpec_df["Youden_Index"] <- Youden_Index

#Identifying cutoff value based on Youden's index
X_point = 1 - SensSpec_df$Specificity[which.max(SensSpec_df$Youden_Index)]
Y_point = SensSpec_df$Sensitivity[which.max(SensSpec_df$Youden_Index)]
point_df = data.frame(X_point, Y_point)

#Visualizing the ROC curve and the cutoff value
ggplot(SensSpec_df, aes(x=1-Specificity, y= Sensitivity)) + geom_line(lwd=1)+geom_point(data= point_df, aes(x = X_point, y = Y_point), color = 'red', size = 3) + annotate("text", x=0.14, y=0.75, label= "Youden's Index", color="red") 


```




To find the cutoff value for the mean area Youden's index was used. Youden's index gives a cutoff value that maximizes both sensitivity and specificity. It gives a cutoff value that has the highest sensitivity + specificity -1. Based on the index, the cutoff value for the mean area is 695.71. Tumors with the mean area above or equal to 695.71 are malignant and those with the mean area below 695.71 are benign.

```{r, echo=FALSE, warning = FALSE, message = FALSE}
#Printing the respective mean area cutoff value
Cut_off =  SensSpec_df$cut_off[which.max(SensSpec_df$Youden_Index)]
print(paste("The derived cutoff mean area is:", round(Cut_off,2)), quote = FALSE)
```

### Histogram and scatterplot for the Mean Area With the Cutoff Value {-}

```{r,fig.height = 4, fig.cap=paste("Histogram and scatter plot with the cutoff value"), fig.width=10, echo=FALSE, warning = FALSE, message = FALSE,fig.align='center', fig.pos="H"}
# Visualizing histogram of mean area by diagnosis together with the cutoff value
p_hist_cutoff <-
  ggplot(breast_cancer_df, aes(x = area_mean, color = diagnosis)) + geom_histogram(
    aes(y = ..density..),
    alpha = 0.5,
    bins = 40,
    show.legend = FALSE
  ) + geom_density() + scale_colour_manual(values = c("M" = "blue", "B" = "red")) + xlab("Index") + ylab("area_mean") + geom_vline(xintercept = 695.71)+ annotate("text", x= 1000, y=0.003, label= "Cutoff Value") 

# Visualizing scatter plot by diagnosis together with the cutoff value
g_scatter_cutoff = ggplot(breast_cancer_df, aes(
  x = seq(1, 569),
  y = area_mean,
  color = diagnosis
)) + geom_point() + scale_colour_manual(values = c("M" = "blue", "B" = "red")) + xlab("Index") + ylab("area_mean") + geom_hline(yintercept = 695.71)+ annotate("text", x=30, y=800, label= "Cutoff Value") 

#Presenting the histogram and the scatter plot side by side 
grid_arrange_shared_legend(p_hist_cutoff, g_scatter_cutoff, ncol =2)

```


### Summary of Performance of the Derived Cutoff {-}

\footnotesize

```{r, echo=FALSE, warning = FALSE, message = FALSE}
#Evaluating the performance of the proposed cutoff value
pred_value = ifelse(breast_cancer_df$area_mean >= Cut_off, 1, 0)
diagnosis_numeric = ifelse(breast_cancer_df$diagnosis == "B", 0, 1)
confusionMatrix(factor(diagnosis_numeric), factor(pred_value), positive = "1")
```
\normalsize

The proposed cutoff for the mean area gave a sensitivity of 94.2% and specificity of 87.4%, meaning the cutoff classified 94.2% of malignant tumor cases and 87.4% of benign tumor cases correctly. The positive predictive value is 76.4%, indicating out of 212 samples classified as malignant tumors, 162 (76.4%) were correct. Also, according to the negative predictive value, out of 357 cases classified as benign tumors, 347(97.2%) were correct. The cutoff misclassified 50 benign tumors as malignant and 10 malignant tumors as benign. Overall, the cutoff correctly classified 509(90.8%) of the samples.  


# Summary

Data analysis was performed to derive a cutoff for the mean area and to find a combination of potentially diagnostic features to explain outcome of breast cancer diagnosis. The dataset comprises 30 features and 569 instances. Both descriptive and predictive analysis were performed. The descriptive analysis made indicated that the distribution of biomarkers are deviated from a normal distribution. The results of Spearman's rank correlation, PCA, and collinearity statistics indicated that the biomarkers have strong pairwise correlations and dependencies. 
<br/> Random forest and logistic regression were used to be able to identify the type of the tumor based on the given biomarkers. The logistic regression used the the first five principal components as predictors. The first five principal components account for 84.8% of the observed variation. For the random forest 15 original biomarkers were used. Those biomarkers were identified by applying feature importance analysis. Then, the performance of the two models were compared. Accordingly, the logistic regression that used the first five principal components outperformed the random forest model that is based on 15 features. 

<br/> Finally, a cutoff value was derived for the mean area. The cutoff was identified using Youden's index. The index returns a value at which point the sum of sensitivity and specificity is the highest. The derived cutoff value is 695.71. The cutoff gave a sensitivity of 94.2% and a specificity of 87.4%.




